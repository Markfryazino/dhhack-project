{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = ['warandpeace.txt', 'annakarenina.txt', 'idiot.txt', 'onegin.txt', 'jivago.txt', 'ktulhu_demo.txt',\n",
    "        'atthemountainsofmadness.txt', 'brotherskaramasovi.txt', 'journeytomoscow.txt', 'oblomov.txt']\n",
    "medium = ['choke.txt', 'fightclub.txt', 'lullaby.txt', 'masterandmargaret.txt', 'messenger.txt',\n",
    "         'pulpfiction.txt', 'survivor.txt', 'oneflewover.txt']\n",
    "slang = ['mitiki.txt', 'rastamans.txt', 'chapaev_demo.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [clean, medium, slang]\n",
    "texts = []\n",
    "fulls = []\n",
    "sets = []\n",
    "\n",
    "for group in names:\n",
    "    docs = []\n",
    "    full = ''\n",
    "    for book in group:\n",
    "        corpus = []\n",
    "        with open('books/' + book) as file:\n",
    "            for line in file:\n",
    "                corpus.append(line)\n",
    "\n",
    "        corpus = [el for el in corpus if el != '\\n']\n",
    "        corpus = [re.sub(r'[^\\w\\s]','',el)[:-1] for el in corpus]\n",
    "        for sentence in corpus:\n",
    "            full += ' ' + sentence\n",
    "        docs += [el.lower().split() for el in corpus]\n",
    "        \n",
    "    words = []\n",
    "    for sentence in docs:\n",
    "        words += sentence\n",
    "    sets.append(set(words))\n",
    "    texts += docs\n",
    "    fulls.append(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(texts, size=100, iter=70, workers=4, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vec = TfidfVectorizer()\n",
    "tfidf = vec.fit_transform(fulls)\n",
    "npidf = tfidf.toarray()\n",
    "fnames = vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logic(string):\n",
    "    for word in string.split():\n",
    "        try:\n",
    "            index = fnames.index(word)\n",
    "            idf = npidf[:, index]\n",
    "            if (idf[1] > 100 * idf[0]) or (idf[2] > 100 * idf[0]):\n",
    "                best = '0'\n",
    "                best_res = -100\n",
    "                for gword in sets[0]:\n",
    "                    score = model.wv.similarity(word, gword)\n",
    "                    if score > best_res:\n",
    "                        best_res = score\n",
    "                        best = gword\n",
    "                print(word, ' ---> ', best)\n",
    "        except:\n",
    "            pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
